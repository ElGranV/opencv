<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tracking API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tracking</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import cv2


TRACKING_TYPE = &#34;CSRT&#34;

class Tracker:
    #--------Static Attributes---------------
    CONFIG = {&#34;DEFAULT_FACE_CASCADE_SCALE_FACTOR&#34;:1.1,
    &#34;DEFAULT_FACE_CASCADE_MIN_NEIGHBORS&#34;:4 ,
    &#34;DEFAULT_HOG_WIN_STRIDE&#34;: (4,4),
    &#34;DEFAULT_HOG_SCALE&#34;:1.2
    }
    #----------------------------------------

    def __init__(self, type = &#34;face&#34;):
        &#34;&#34;&#34;
        Cette classe doit permettre de simplifier le tracking d&#39;une personne en train de parler devant une caméra, 
        ou en train de danser. Peut-être utilisé dans le cadre d&#39;un système de suivi dynamique par caméra.
        Le fonctionnement allie détection automatique, soit par des alogrithmes de SVM (dans le cas d&#39;une personne entièrement visible,
        on utilisee cv2.HOGDescriptor_getDefaultPeopleDetector) ou avec des modèles de reconnaissance faciale (dans le cas d&#39;une personne proche
        de la caméra), et des méthodes d&#39;Object Tracking.
        L&#39;idée est de définir périodiquement une région ou se situe la personne suivie (par SVM ou face_cascade) puis de suivre cette
        région via un tracking CSRT, qui est beaucoup moins gourmand donc plus rapide, mais aussi moins précis.
        L&#39;idée est que le tracking fonctionne en temps réel, et permette donc de piloter une action mécanique (comme un mouvement de servomoteur)
        en fonction de la position de la personne dans l&#39;image.
        
        -----------------
        Attributes:
            box_tracker:
                Object Tracker de type CSRT
            frame_count: int
                compteur de frame
            detection_type: str
                type de detection (face ou person)
        
        -----------------
        ..todo::Ajouter l&#39;évaluation de la distance
        &#34;&#34;&#34;
        self.box_tracker = cv2.TrackerCSRT_create()
        self.frame_count = 0
        self.detection_type = type
        self.launch_detection = True
        
        if type == &#34;face&#34;:
            self.face_cascade = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;)
        if type == &#34;person&#34;:
            # initialisation du HOG:
            self.hog = cv2.HOGDescriptor()
            self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        
        self.config = {

        }
        

    
    def init(self, video_path = 0, detection_rate = 50):
        &#34;&#34;&#34;
        Initialisation du tracker.
        -   Ouverture du flux vidéo. 
        -   Initialisation des valeurs
        
        Parameters
        ---------
        video_path: str
            Chemin vers le flux video. S&#39;il vaut 0, cela ouvre la webcam. Peut lancer une exception en cas d&#39;échec.
        detection_rate: int
            Nombre de frame au bout desquelles le script réenclenche une détection. Plus ce nombre est élevé plus le soin du tracking 
            est laissé au box tracking. Le box tracking étant moins gourmand que la detection faciale ou de pietons, c&#39;est un équilibre à trouver
            en fonction du fps de la camera, pour ne pas trop ralentir le système. 
        &#34;&#34;&#34;
        self.video = cv2.VideoCapture(video_path)
        if not self.video.isOpened():
            raise(RuntimeError(&#34;Erreur à l&#39;ouverture du flux video&#34;))
        if detection_rate &gt; 0: self.detection_rate = detection_rate
        else: self.detection_rate = 50
    
    
    def update(self, show = False):
        &#34;&#34;&#34;
        Met à jour la bounding box.
        &#34;&#34;&#34;
        #on récupère une frame du flux video
        ok, frame = self.video.read()
        if ok:
            #On redimensionne la taille de la frame et on la passe en noir et blanc pour accélérer la détection
            frame = cv2.resize(frame, (640, 480))
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            
            #On incrémente le compteur de frame
            self.frame_count += 1
            #Si le compteur de frame est un multiple detection_rate on lance la détection
            if (self.frame_count-1) % self.detection_rate == 0: self.launch_detection = True
            
            if self.launch_detection:
                boxes = self.detect(gray_frame)[&#34;boxes&#34;]
                if len(boxes):
                    bbox = tuple(boxes[0])
                    success = True
                    #On initialise le &#34;box_tracker&#34; (object tracker) avec la box issue de la détection (face_cascade ou hog)
                    self.box_tracker.init(gray_frame, bbox)
                    self.launch_detection = False
                else:
                    success = False
                    bbox = None
                    
            else:
                success, bbox = self.box_tracker.update(frame)
                if not success: self.launch_detection = True
            
            self.frame_count += 1
            self.frame_count = self.frame_count % 100000
            
            if success: self.bbox = bbox
            #On peut afficher les images
            if show: self.show(frame, bbox, success)
            return [success, bbox]
        return [False, None]



    
    def detect(self, frame):
        &#34;&#34;&#34;
        Parameters
        ------------
        frame: any
            frame/image provenant du flux vidéo (video.read())

        Returns
        -----------
        dict: dict
            {&#34;boxes&#34;: liste de boxes où des individus ont été détectés}
        &#34;&#34;&#34;
        if self.detection_type == &#34;face&#34;:
            return{&#34;boxes&#34;:self.face_cascade.detectMultiScale(frame, scaleFactor = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_SCALE_FACTOR&#34;], 
            minNeighbors = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_MIN_NEIGHBORS&#34;])}
        
        if self.detection_type == &#34;person&#34;:
            boxes, weights = self.hog.detectMultiScale(frame, winStride = Tracker.CONFIG[&#34;DEFAULT_HOG_WIN_STRIDE&#34;], 
            scale=Tracker.CONFIG[&#34;DEFAULT_HOG_SCALE&#34;])
            return {&#34;boxes&#34;: boxes, &#34;weights&#34;: weights}
        
        return {&#34;boxes&#34;:[]}
    
    def show(self, frame, bbox, success):
        # Draw bounding box
        if success:
            # Tracking success
            p1 = (int(bbox[0]), int(bbox[1]))
            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))
            cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)
        else :
            # Tracking failure
            cv2.putText(frame, &#34;Tracking failure detected&#34;, (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)

        # Display tracker type on frame
        cv2.putText(frame, &#34;Tracker&#34;, (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);
        cv2.imshow(&#39;frame&#39;, frame)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tracking.Tracker"><code class="flex name class">
<span>class <span class="ident">Tracker</span></span>
<span>(</span><span>type='face')</span>
</code></dt>
<dd>
<div class="desc"><p>Cette classe doit permettre de simplifier le tracking d'une personne en train de parler devant une caméra,
ou en train de danser. Peut-être utilisé dans le cadre d'un système de suivi dynamique par caméra.
Le fonctionnement allie détection automatique, soit par des alogrithmes de SVM (dans le cas d'une personne entièrement visible,
on utilisee cv2.HOGDescriptor_getDefaultPeopleDetector) ou avec des modèles de reconnaissance faciale (dans le cas d'une personne proche
de la caméra), et des méthodes d'Object Tracking.
L'idée est de définir périodiquement une région ou se situe la personne suivie (par SVM ou face_cascade) puis de suivre cette
région via un tracking CSRT, qui est beaucoup moins gourmand donc plus rapide, mais aussi moins précis.
L'idée est que le tracking fonctionne en temps réel, et permette donc de piloter une action mécanique (comme un mouvement de servomoteur)
en fonction de la position de la personne dans l'image.</p>
<hr>
<h2 id="attributes">Attributes</h2>
<dl>
<dt>box_tracker:</dt>
<dt>Object Tracker de type CSRT</dt>
<dt><strong><code>frame_count</code></strong></dt>
<dd>int
compteur de frame</dd>
<dt><strong><code>detection_type</code></strong></dt>
<dd>str
type de detection (face ou person)</dd>
</dl>
<hr>
<div class="admonition todo">
<p class="admonition-title">TODO</p>
<p>Ajouter l'évaluation de la distance</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tracker:
    #--------Static Attributes---------------
    CONFIG = {&#34;DEFAULT_FACE_CASCADE_SCALE_FACTOR&#34;:1.1,
    &#34;DEFAULT_FACE_CASCADE_MIN_NEIGHBORS&#34;:4 ,
    &#34;DEFAULT_HOG_WIN_STRIDE&#34;: (4,4),
    &#34;DEFAULT_HOG_SCALE&#34;:1.2
    }
    #----------------------------------------

    def __init__(self, type = &#34;face&#34;):
        &#34;&#34;&#34;
        Cette classe doit permettre de simplifier le tracking d&#39;une personne en train de parler devant une caméra, 
        ou en train de danser. Peut-être utilisé dans le cadre d&#39;un système de suivi dynamique par caméra.
        Le fonctionnement allie détection automatique, soit par des alogrithmes de SVM (dans le cas d&#39;une personne entièrement visible,
        on utilisee cv2.HOGDescriptor_getDefaultPeopleDetector) ou avec des modèles de reconnaissance faciale (dans le cas d&#39;une personne proche
        de la caméra), et des méthodes d&#39;Object Tracking.
        L&#39;idée est de définir périodiquement une région ou se situe la personne suivie (par SVM ou face_cascade) puis de suivre cette
        région via un tracking CSRT, qui est beaucoup moins gourmand donc plus rapide, mais aussi moins précis.
        L&#39;idée est que le tracking fonctionne en temps réel, et permette donc de piloter une action mécanique (comme un mouvement de servomoteur)
        en fonction de la position de la personne dans l&#39;image.
        
        -----------------
        Attributes:
            box_tracker:
                Object Tracker de type CSRT
            frame_count: int
                compteur de frame
            detection_type: str
                type de detection (face ou person)
        
        -----------------
        ..todo::Ajouter l&#39;évaluation de la distance
        &#34;&#34;&#34;
        self.box_tracker = cv2.TrackerCSRT_create()
        self.frame_count = 0
        self.detection_type = type
        self.launch_detection = True
        
        if type == &#34;face&#34;:
            self.face_cascade = cv2.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;)
        if type == &#34;person&#34;:
            # initialisation du HOG:
            self.hog = cv2.HOGDescriptor()
            self.hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        
        self.config = {

        }
        

    
    def init(self, video_path = 0, detection_rate = 50):
        &#34;&#34;&#34;
        Initialisation du tracker.
        -   Ouverture du flux vidéo. 
        -   Initialisation des valeurs
        
        Parameters
        ---------
        video_path: str
            Chemin vers le flux video. S&#39;il vaut 0, cela ouvre la webcam. Peut lancer une exception en cas d&#39;échec.
        detection_rate: int
            Nombre de frame au bout desquelles le script réenclenche une détection. Plus ce nombre est élevé plus le soin du tracking 
            est laissé au box tracking. Le box tracking étant moins gourmand que la detection faciale ou de pietons, c&#39;est un équilibre à trouver
            en fonction du fps de la camera, pour ne pas trop ralentir le système. 
        &#34;&#34;&#34;
        self.video = cv2.VideoCapture(video_path)
        if not self.video.isOpened():
            raise(RuntimeError(&#34;Erreur à l&#39;ouverture du flux video&#34;))
        if detection_rate &gt; 0: self.detection_rate = detection_rate
        else: self.detection_rate = 50
    
    
    def update(self, show = False):
        &#34;&#34;&#34;
        Met à jour la bounding box.
        &#34;&#34;&#34;
        #on récupère une frame du flux video
        ok, frame = self.video.read()
        if ok:
            #On redimensionne la taille de la frame et on la passe en noir et blanc pour accélérer la détection
            frame = cv2.resize(frame, (640, 480))
            gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            
            #On incrémente le compteur de frame
            self.frame_count += 1
            #Si le compteur de frame est un multiple detection_rate on lance la détection
            if (self.frame_count-1) % self.detection_rate == 0: self.launch_detection = True
            
            if self.launch_detection:
                boxes = self.detect(gray_frame)[&#34;boxes&#34;]
                if len(boxes):
                    bbox = tuple(boxes[0])
                    success = True
                    #On initialise le &#34;box_tracker&#34; (object tracker) avec la box issue de la détection (face_cascade ou hog)
                    self.box_tracker.init(gray_frame, bbox)
                    self.launch_detection = False
                else:
                    success = False
                    bbox = None
                    
            else:
                success, bbox = self.box_tracker.update(frame)
                if not success: self.launch_detection = True
            
            self.frame_count += 1
            self.frame_count = self.frame_count % 100000
            
            if success: self.bbox = bbox
            #On peut afficher les images
            if show: self.show(frame, bbox, success)
            return [success, bbox]
        return [False, None]



    
    def detect(self, frame):
        &#34;&#34;&#34;
        Parameters
        ------------
        frame: any
            frame/image provenant du flux vidéo (video.read())

        Returns
        -----------
        dict: dict
            {&#34;boxes&#34;: liste de boxes où des individus ont été détectés}
        &#34;&#34;&#34;
        if self.detection_type == &#34;face&#34;:
            return{&#34;boxes&#34;:self.face_cascade.detectMultiScale(frame, scaleFactor = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_SCALE_FACTOR&#34;], 
            minNeighbors = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_MIN_NEIGHBORS&#34;])}
        
        if self.detection_type == &#34;person&#34;:
            boxes, weights = self.hog.detectMultiScale(frame, winStride = Tracker.CONFIG[&#34;DEFAULT_HOG_WIN_STRIDE&#34;], 
            scale=Tracker.CONFIG[&#34;DEFAULT_HOG_SCALE&#34;])
            return {&#34;boxes&#34;: boxes, &#34;weights&#34;: weights}
        
        return {&#34;boxes&#34;:[]}
    
    def show(self, frame, bbox, success):
        # Draw bounding box
        if success:
            # Tracking success
            p1 = (int(bbox[0]), int(bbox[1]))
            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))
            cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)
        else :
            # Tracking failure
            cv2.putText(frame, &#34;Tracking failure detected&#34;, (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)

        # Display tracker type on frame
        cv2.putText(frame, &#34;Tracker&#34;, (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);
        cv2.imshow(&#39;frame&#39;, frame)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="tracking.Tracker.CONFIG"><code class="name">var <span class="ident">CONFIG</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tracking.Tracker.detect"><code class="name flex">
<span>def <span class="ident">detect</span></span>(<span>self, frame)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>frame</code></strong> :&ensp;<code>any</code></dt>
<dd>frame/image provenant du flux vidéo (video.read())</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>{"boxes": liste de boxes où des individus ont été détectés}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect(self, frame):
    &#34;&#34;&#34;
    Parameters
    ------------
    frame: any
        frame/image provenant du flux vidéo (video.read())

    Returns
    -----------
    dict: dict
        {&#34;boxes&#34;: liste de boxes où des individus ont été détectés}
    &#34;&#34;&#34;
    if self.detection_type == &#34;face&#34;:
        return{&#34;boxes&#34;:self.face_cascade.detectMultiScale(frame, scaleFactor = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_SCALE_FACTOR&#34;], 
        minNeighbors = Tracker.CONFIG[&#34;DEFAULT_FACE_CASCADE_MIN_NEIGHBORS&#34;])}
    
    if self.detection_type == &#34;person&#34;:
        boxes, weights = self.hog.detectMultiScale(frame, winStride = Tracker.CONFIG[&#34;DEFAULT_HOG_WIN_STRIDE&#34;], 
        scale=Tracker.CONFIG[&#34;DEFAULT_HOG_SCALE&#34;])
        return {&#34;boxes&#34;: boxes, &#34;weights&#34;: weights}
    
    return {&#34;boxes&#34;:[]}</code></pre>
</details>
</dd>
<dt id="tracking.Tracker.init"><code class="name flex">
<span>def <span class="ident">init</span></span>(<span>self, video_path=0, detection_rate=50)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialisation du tracker.
-
Ouverture du flux vidéo.
-
Initialisation des valeurs</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>video_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Chemin vers le flux video. S'il vaut 0, cela ouvre la webcam. Peut lancer une exception en cas d'échec.</dd>
<dt><strong><code>detection_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Nombre de frame au bout desquelles le script réenclenche une détection. Plus ce nombre est élevé plus le soin du tracking
est laissé au box tracking. Le box tracking étant moins gourmand que la detection faciale ou de pietons, c'est un équilibre à trouver
en fonction du fps de la camera, pour ne pas trop ralentir le système.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init(self, video_path = 0, detection_rate = 50):
    &#34;&#34;&#34;
    Initialisation du tracker.
    -   Ouverture du flux vidéo. 
    -   Initialisation des valeurs
    
    Parameters
    ---------
    video_path: str
        Chemin vers le flux video. S&#39;il vaut 0, cela ouvre la webcam. Peut lancer une exception en cas d&#39;échec.
    detection_rate: int
        Nombre de frame au bout desquelles le script réenclenche une détection. Plus ce nombre est élevé plus le soin du tracking 
        est laissé au box tracking. Le box tracking étant moins gourmand que la detection faciale ou de pietons, c&#39;est un équilibre à trouver
        en fonction du fps de la camera, pour ne pas trop ralentir le système. 
    &#34;&#34;&#34;
    self.video = cv2.VideoCapture(video_path)
    if not self.video.isOpened():
        raise(RuntimeError(&#34;Erreur à l&#39;ouverture du flux video&#34;))
    if detection_rate &gt; 0: self.detection_rate = detection_rate
    else: self.detection_rate = 50</code></pre>
</details>
</dd>
<dt id="tracking.Tracker.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self, frame, bbox, success)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show(self, frame, bbox, success):
    # Draw bounding box
    if success:
        # Tracking success
        p1 = (int(bbox[0]), int(bbox[1]))
        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))
        cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)
    else :
        # Tracking failure
        cv2.putText(frame, &#34;Tracking failure detected&#34;, (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)

    # Display tracker type on frame
    cv2.putText(frame, &#34;Tracker&#34;, (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);
    cv2.imshow(&#39;frame&#39;, frame)</code></pre>
</details>
</dd>
<dt id="tracking.Tracker.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, show=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Met à jour la bounding box.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, show = False):
    &#34;&#34;&#34;
    Met à jour la bounding box.
    &#34;&#34;&#34;
    #on récupère une frame du flux video
    ok, frame = self.video.read()
    if ok:
        #On redimensionne la taille de la frame et on la passe en noir et blanc pour accélérer la détection
        frame = cv2.resize(frame, (640, 480))
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        
        #On incrémente le compteur de frame
        self.frame_count += 1
        #Si le compteur de frame est un multiple detection_rate on lance la détection
        if (self.frame_count-1) % self.detection_rate == 0: self.launch_detection = True
        
        if self.launch_detection:
            boxes = self.detect(gray_frame)[&#34;boxes&#34;]
            if len(boxes):
                bbox = tuple(boxes[0])
                success = True
                #On initialise le &#34;box_tracker&#34; (object tracker) avec la box issue de la détection (face_cascade ou hog)
                self.box_tracker.init(gray_frame, bbox)
                self.launch_detection = False
            else:
                success = False
                bbox = None
                
        else:
            success, bbox = self.box_tracker.update(frame)
            if not success: self.launch_detection = True
        
        self.frame_count += 1
        self.frame_count = self.frame_count % 100000
        
        if success: self.bbox = bbox
        #On peut afficher les images
        if show: self.show(frame, bbox, success)
        return [success, bbox]
    return [False, None]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tracking.Tracker" href="#tracking.Tracker">Tracker</a></code></h4>
<ul class="">
<li><code><a title="tracking.Tracker.CONFIG" href="#tracking.Tracker.CONFIG">CONFIG</a></code></li>
<li><code><a title="tracking.Tracker.detect" href="#tracking.Tracker.detect">detect</a></code></li>
<li><code><a title="tracking.Tracker.init" href="#tracking.Tracker.init">init</a></code></li>
<li><code><a title="tracking.Tracker.show" href="#tracking.Tracker.show">show</a></code></li>
<li><code><a title="tracking.Tracker.update" href="#tracking.Tracker.update">update</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>